{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49b36a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42af0834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39a78ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16348\\3145715120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507dfb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from nltk import word_tokenize\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyarrow as pa # for creating spark dataframe\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912f50a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/muddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7258efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out', 'we', 'was', 'how', 'myself', 'for', 'they', 'about', \"hasn't\", 'then', 'both', 'so', 're', 'don', 'm', 'as', 'any', 'mightn', 'after', 'you', 'wouldn', 'why', 'been', 'where', 'by', \"isn't\", 'yourself', 'wasn', 'a', \"haven't\", 'did', \"hadn't\", 'their', 'hasn', 'doing', 'be', 'further', 'ours', 'now', 'am', 'her', \"you'll\", 'yourselves', 'that', 'my', 'what', 'to', 'd', 'not', \"won't\", \"couldn't\", 'own', 'there', 'this', 'each', 'all', 'haven', 'more', 'me', 've', 'weren', 'which', 'himself', 'nor', 'other', \"shouldn't\", 'who', \"should've\", 'same', 'at', 'such', 't', 'up', 'than', 'can', \"you've\", 'too', 'these', 'while', \"wasn't\", 'ourselves', 'before', 'i', 'he', \"didn't\", 'our', 'its', 'but', 'with', \"wouldn't\", 'those', 'because', 'the', 'y', 'shouldn', 'it', 'mustn', 'hers', 'just', 'doesn', 'ain', 'between', 'over', 'had', 'aren', \"mightn't\", 'does', 'have', 'and', 'or', 'some', \"mustn't\", 'only', 'won', 'when', 'needn', 'below', 'in', 'if', 'theirs', \"needn't\", \"aren't\", 'isn', 'again', 'his', 'whom', 'll', 'hadn', 'above', 'should', 'itself', 'themselves', 'until', 'are', 'she', 'no', 'from', 'into', 'will', 'your', 'few', 'herself', 'of', 'has', 'down', 'were', 'once', 'ma', 'having', 'them', 'under', 'him', 'shan', 'couldn', 'do', 'on', 'an', \"you'd\", 'yours', 'being', 'off', 'o', \"that'll\", 'very', \"weren't\", 'didn', 'through', \"you're\", 'most', 'against', \"it's\", \"doesn't\", 'here', 'is', 's', \"don't\", \"shan't\", 'during', \"she's\"}\n"
     ]
    }
   ],
   "source": [
    "# Print list of stopwords for informational purposes\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)\n",
    "# We might consider removing some of these or making our own list since our text is weird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b38e1",
   "metadata": {},
   "source": [
    "<a href=\"https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\" target=\"_blank\">Pandas cheat sheet</a><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d063d9",
   "metadata": {},
   "source": [
    "<A HREF=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">Pyspark ML guide</A><BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fecf73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"messages.csv\")\n",
    "# Replace message NaNs with zero length string ''\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fa63877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#summit1g\n"
     ]
    }
   ],
   "source": [
    "# Only for 2 channel's of data\n",
    "# codes channel in 1st row as 0, other channel is 1\n",
    "df['target'] = pd.DataFrame(np.where(df.channel==df.channel[0], 0, 1))\n",
    "print(df.channel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b61053",
   "metadata": {},
   "source": [
    "<a href=\"https://www.programiz.com/python-programming/list-comprehension\" target=\"_blank\">List comprehension</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6ecc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "df['tokens'] = [word_tokenize(string) for string in df['message']]\n",
    "df['tokens'] = [[word.lower() for word in token] for token in df['tokens'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70161c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "df['tokens'] = [[word for word in token if word not in stopwords.words('english') ] for token in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73426fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blanks ''\n",
    "df['tokens'] = [[word for word in token if len(word) > 0] for token in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1a99e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of words in tokens\n",
    "df['count'] = [len(token) for token in df['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d53371",
   "metadata": {},
   "source": [
    "Since many of these aren't words, I think stemming and lemmatization are not useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8b4474b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>channel</th>\n",
       "      <th>message</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moonbootsies</td>\n",
       "      <td>#summit1g</td>\n",
       "      <td>monkaW Cam we can talk about this</td>\n",
       "      <td>0</td>\n",
       "      <td>[monkaw, cam, talk]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superflacken</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>EZ</td>\n",
       "      <td>1</td>\n",
       "      <td>[ez]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>madnessbr3210</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>ez</td>\n",
       "      <td>1</td>\n",
       "      <td>[ez]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brlckbruh</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>EZ</td>\n",
       "      <td>1</td>\n",
       "      <td>[ez]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ryker2_</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>PagMan 󠀀</td>\n",
       "      <td>1</td>\n",
       "      <td>[pagman, 󠀀]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>angerog</td>\n",
       "      <td>#summit1g</td>\n",
       "      <td>does he play with clutch?</td>\n",
       "      <td>0</td>\n",
       "      <td>[play, clutch, ?]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>biscuit_090</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>xqcMald</td>\n",
       "      <td>1</td>\n",
       "      <td>[xqcmald]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823</th>\n",
       "      <td>welshmaster91</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>PotFriend PotFriend &lt;3</td>\n",
       "      <td>1</td>\n",
       "      <td>[potfriend, potfriend, &lt;, 3]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>acchampy</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>xqcMald</td>\n",
       "      <td>1</td>\n",
       "      <td>[xqcmald]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>temp6t</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>xqcMald</td>\n",
       "      <td>1</td>\n",
       "      <td>[xqcmald]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3826 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           username    channel                            message  target  \\\n",
       "0      moonbootsies  #summit1g  monkaW Cam we can talk about this       0   \n",
       "1      superflacken     #xqcow                                 EZ       1   \n",
       "2     madnessbr3210     #xqcow                                 ez       1   \n",
       "3         brlckbruh     #xqcow                                 EZ       1   \n",
       "4           ryker2_     #xqcow                           PagMan 󠀀       1   \n",
       "...             ...        ...                                ...     ...   \n",
       "3821        angerog  #summit1g          does he play with clutch?       0   \n",
       "3822    biscuit_090     #xqcow                            xqcMald       1   \n",
       "3823  welshmaster91     #xqcow             PotFriend PotFriend <3       1   \n",
       "3824       acchampy     #xqcow                            xqcMald       1   \n",
       "3825         temp6t     #xqcow                            xqcMald       1   \n",
       "\n",
       "                            tokens  count  \n",
       "0              [monkaw, cam, talk]      3  \n",
       "1                             [ez]      1  \n",
       "2                             [ez]      1  \n",
       "3                             [ez]      1  \n",
       "4                      [pagman, 󠀀]      2  \n",
       "...                            ...    ...  \n",
       "3821             [play, clutch, ?]      3  \n",
       "3822                     [xqcmald]      1  \n",
       "3823  [potfriend, potfriend, <, 3]      4  \n",
       "3824                     [xqcmald]      1  \n",
       "3825                     [xqcmald]      1  \n",
       "\n",
       "[3826 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bcc86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch0 = df.query('target == 0')\n",
    "ch1 = df.query('target == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8263f6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>channel</th>\n",
       "      <th>message</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superflacken</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>EZ</td>\n",
       "      <td>1</td>\n",
       "      <td>[ez]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>madnessbr3210</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>ez</td>\n",
       "      <td>1</td>\n",
       "      <td>[ez]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brlckbruh</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>EZ</td>\n",
       "      <td>1</td>\n",
       "      <td>[ez]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ryker2_</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>PagMan 󠀀</td>\n",
       "      <td>1</td>\n",
       "      <td>[pagman, 󠀀]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>refluence</td>\n",
       "      <td>#xqcow</td>\n",
       "      <td>5Head</td>\n",
       "      <td>1</td>\n",
       "      <td>[5head]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username channel   message  target       tokens  count\n",
       "1   superflacken  #xqcow        EZ       1         [ez]      1\n",
       "2  madnessbr3210  #xqcow        ez       1         [ez]      1\n",
       "3      brlckbruh  #xqcow        EZ       1         [ez]      1\n",
       "4        ryker2_  #xqcow  PagMan 󠀀       1  [pagman, 󠀀]      2\n",
       "5      refluence  #xqcow     5Head       1      [5head]      1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2557e92",
   "metadata": {},
   "source": [
    "OK, try to make a Bag-O-Words for both channels' tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aeed6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords0 = [[word for word in token[0].split() ] for token in ch0['tokens'] if len(token)>0]\n",
    "# flat_list = [item for sublist in list_of_lists for item in sublist]\n",
    "allwords0 = np.unique(np.array([item for sublist in allwords0 for item in sublist]))\n",
    "allwords1 = [[word for word in token[0].split() ] for token in ch1['tokens'] if len(token)>0]\n",
    "# flat_list = [item for sublist in list_of_lists for item in sublist]\n",
    "allwords1 = np.unique(np.array([item for sublist in allwords1 for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acfda0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok let's make a master list of all words\n",
    "allwords = [[word for word in token[0].split() ] for token in df['tokens'] if len(token)>0 ]\n",
    "# flat_list = [item for sublist in list_of_lists for item in sublist]\n",
    "allwords = np.unique(np.array([item for sublist in allwords for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83acd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x01action' '!' '#' '$' \"''\" \"'re\" \"'s\" '+1' '-1' '.' '..' '...' '....'\n",
      " '.....' '..........' '..............' '0.42' '1' '1/3' '10k' '1:16' '2'\n",
      " '2/6' '3' '3.6' '3/6' '3head' '4heed' '4shrug' '5/6' '5/7' '5head' '5hed'\n",
      " '720p' '76' '8x' ':' ';' '<' '>' '?' '@' 'aaaaaaaaaaaaaaaaaaad'\n",
      " 'aaaaware' 'absolutely' 'actually' 'ad' 'add' 'adddddddddddddd' 'adhd'\n",
      " 'ads' 'ahh' 'alienpls3' 'aloonidrive' 'android' 'another' 'anyopne'\n",
      " 'arriveeeeeeeed' 'ass' 'atleast' 'auuuuuugh' 'aware' 'ayaya' 'ayoooo' 'b'\n",
      " 'back' 'backrooms' 'bait' 'basically' 'batchest' 'batpls' 'bcwarrior'\n",
      " 'best' 'better' 'biblethump' 'big' 'binoculars' 'blue' 'blues' 'bocket'\n",
      " 'bono' 'boom' 'booooom' 'bop' 'boss' 'brain' 'brksteer' 'bro' 'brook'\n",
      " 'broom' 'broomcloset' 'bruh' 'bucket' 'bucketception' 'bucketceptionn'\n",
      " 'bucketfriend' 'buckettttt' 'bullied' 'burn' 'buy' 'cake' 'calm' 'cam'\n",
      " 'came' 'cameron' 'cancel' 'car' 'chase' 'chat' 'chatting' 'childish'\n",
      " 'chill' 'chills' 'clap' 'clean' 'close' 'closet' 'closing' 'cloudlol'\n",
      " 'clue' 'clueless' 'collectible' 'come' 'comeon' 'commentator' 'comp'\n",
      " 'companion' 'content' 'coolstorybob' 'copege' 'copium' 'copying'\n",
      " 'countersteer' 'cre' 'cringe' 'csgo' 'cum' 'damn' 'dansgame' 'dare'\n",
      " 'despair' 'destroy' 'deww' 'dl' 'dont' 'door' 'doubters' 'dream' 'drop'\n",
      " 'dude' 'dudes' 'eggypog' 'elisd' 'elisnerd' 'elisweird' 'eliswow' 'emote'\n",
      " 'emotional' 'empid' 'epogu' 'etf' 'ez' 'ezy' 'f' 'fac' 'factory' 'facts'\n",
      " 'fake' 'false' 'far' 'feel' 'feelsbadman' 'feelsdankman' 'feelsrainman'\n",
      " 'feelsstrong' 'feelsstrongman' 'feelsweirdman' 'feelsweirdmanw' 'felix'\n",
      " 'fill' 'finally' 'find' 'five' 'fk' 'follow' 'forsencd' 'forsenthink'\n",
      " 'fuck' 'fucker' 'fuckin' 'fucking' 'fukko' 'fungineer' 'funko' 'fuukkk'\n",
      " 'g' 'gachigasm' 'gachihyper' 'game' 'gaming' 'get' 'ggoooo' 'gib'\n",
      " 'gifted' 'gigachad' 'give' 'gloves' 'go' 'god' 'goes' 'gogogog' 'good'\n",
      " 'goofy' 'google' 'goooooo' 'gooooooo' 'got' 'gotem' 'gottem' 'gotttemmmm'\n",
      " 'gp' 'grek' 'guy' 'haahaha' 'hahaha' 'hand' 'happen' 'hard' 'harry'\n",
      " 'hate' 'hell' 'hes' 'hey' 'hh' 'hi' 'hold' 'holy' 'honestly' 'hows'\n",
      " 'https' 'huge' 'huh' 'hunt' 'hypers' 'ibuypower' 'ice' 'idfk' 'idk'\n",
      " 'iknside' 'imagine' 'impatient' 'important' 'insane' 'iphone' 'ithink'\n",
      " 'j' 'jebaited' 'jeeeesus' 'job' 'join' 'kappa' 'kappapride' 'keep' 'kekl'\n",
      " 'kekw' 'kkonaw' 'komodohype' 'kreygasm' 'l' 'lacomega' 'leave'\n",
      " 'lebronjam' 'left' 'legendary' 'lego' 'lets' 'letsgo' 'lfg' 'like' 'lisa'\n",
      " 'list' 'literally' 'lmao' 'lol' 'longchamp' 'looove' 'luigi' 'lul' 'lulw'\n",
      " 'lupuslycan' 'mad' 'madge' 'madgeknife' 'malding' 'mans' 'many' 'maxlol'\n",
      " 'mazda' 'mean' 'megalul' 'mendocomfy' 'mendosip' 'mf' 'mhm' 'mine'\n",
      " 'mini-stans' 'minted' 'mirror' 'modcheck' 'moderator' 'monka' 'monkaeyes'\n",
      " 'monkagiga' 'monkahmm' 'monkalaugh' 'monkas' 'monkashake' 'monkasteer'\n",
      " 'monkaw' 'monkax' 'mop' 'mr' 'much' 'must' 'mx5' 'myavatar' \"n't\" 'na'\n",
      " 'nah' 'nam1okayeg' 'name' 'narrator' 'naw' 'ne' 'need' 'nerd' 'never'\n",
      " 'new' 'nft' 'nice' 'nicesave' 'nkopog' 'noice' 'noise' 'non' 'noo'\n",
      " 'noooo' 'nooooooo' 'nooooooooooo' 'nooooooooooooo' 'nopers' 'notlikethis'\n",
      " 'nvm' 'oddly' 'oh' 'ok' 'okayeg' 'omegalul' 'omegalulbruh' 'omei' 'omg'\n",
      " 'oops' 'outtapocket' 'overwatch' 'ow' 'ow2' 'p1' 'p2' 'pagchomp' 'pagman'\n",
      " 'parasocial' 'pausechamp' 'peelaugh' 'peepoglad' 'peeposad' 'pepeagony'\n",
      " 'pepega' 'pepegachat' 'pepegapls' 'pepehands' 'pepelaugh' 'pepemeltdown'\n",
      " 'pepepains' 'pepog' 'pepopopcorn' 'perfect' 'play' 'played' 'please'\n",
      " 'pls' 'pog' 'pogchamp' 'poggers' 'pogggers' 'pogu' 'pogu2/6' 'poooog'\n",
      " 'pooooogggers' 'poop' 'poopoooooog' 'porosad' 'posed' 'potfriend'\n",
      " 'ppoverheat' 'pre' 'pre-played' 'preplayed' 'press' 'prewatched'\n",
      " 'prewatcher' 'progress' 'punoko' 'push' 'r' 'rare' 'read' 'real'\n",
      " 'reminds' 'respectfully' 'reward' 'rgb' 'right' 'rikkuactually'\n",
      " 'rikkushrug' 'rip' 'ripbozo' 'rude' 'rust' 'sadge' 'safety' 'sandfir1'\n",
      " 'save' 'saved' 'say' 'screw' 'secret' 'see' 'seemsgood' 'sequel' 'sez'\n",
      " 'sheep' 'shit' 'shot' 'side' 'six' 'skin' 'skins' 'skip' 'slam' 'slow'\n",
      " 'smoking' 'sodad' 'soge' 'someone' 'soon' 'souljorn' 'speeders'\n",
      " 'speedrun' 'speedrunning' 'spralihehehe' 'stan' 'stands' 'stanely'\n",
      " 'staniel' 'stanley' 'stanlurine' 'stanlurines' 'stanners' 'stannys'\n",
      " 'stans' 'started' 'stay' 'steer' 'sticker' 'stickers' 'stiggy' 'still'\n",
      " 'stop' 'stream' 'striimmers' 'stya' 'suiiiii' 'sum1g' 'sumez' 'sumg'\n",
      " 'sumlove' 'summit' 'sumsmash' 'sumsteer' 'sumthump' 'sup' 'surely'\n",
      " 'sxlaslive' 'sypherstrong' 'take' 'tangerines' 'tel' 'tell' 'tf' 'thats'\n",
      " 'theres' 'throw' 'ti' 'tires' 'tomar' 'toxic' 'trieasy' 'trolldespair'\n",
      " 'tru' 'true' 'try' 'ttours' 'twat' 'ty' 'tyres' 'u' 'ugh' 'ugly' 'unlesh'\n",
      " 'uwu' 'vicksyd' 'vicksypog' 'vicksyrage' 'vtuber' 'w' 'wait' 'want'\n",
      " 'watched' 'watchinf' 'way' 'waytoodank' 'webcam' 'weebrun' 'weedderage'\n",
      " 'weirdchamp' 'well' 'whad' 'wheres' 'whipwiggle' 'whisker' 'whoa' 'whyyy'\n",
      " 'wicked' 'wickedsteer' 'widepeepohappy' 'widepeeposad' 'wierdchamp'\n",
      " 'wikez' 'wilson' 'winnable' 'woah' 'wokege' 'woooooo' 'woopshmm' 'worst'\n",
      " 'wow' 'wowowowow' 'wtf' 'x' 'x2' 'xd' 'xqc' 'xqcadhd' 'xqcarm1'\n",
      " 'xqccheer' 'xqccry' 'xqcdespair' 'xqce' 'xqcez' 'xqcfinger' 'xqcflushed'\n",
      " 'xqchands' 'xqchyperf' 'xqcl' 'xqcll' 'xqclook' 'xqcm' 'xqcmald'\n",
      " 'xqcmald🖕' 'xqcmom' 'xqcnom' 'xqcnom_hf' 'xqcomega' 'xqcow' 'xqcpeepo'\n",
      " 'xqcrage' 'xqcscoots' 'xqcskip' 'xqcskull' 'xqcsmash' 'xqcsmile'\n",
      " 'xqcstare' 'xqcstare_sg' 'xqcsus' 'xqct' 'xqctake' 'xqctf_sg' 'xqctl'\n",
      " 'xqcy' 'yayayyay' 'yea' 'yeaaaaaaaaaa' 'yeah' 'yell' 'yeppers' 'yessss'\n",
      " 'yo' 'yoo' 'yup' 'zoilnerd' '’' '♫' '🍋' '🐑' '💁\\u200d♂️🪣' '💢' '😂' '😡' '😦'\n",
      " '😳' '🤓' '🧹' '🩰' '🪣']\n"
     ]
    }
   ],
   "source": [
    "print(allwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703ff02",
   "metadata": {},
   "source": [
    "## Pick it up here... need to figure out why this doesn't work for ch1???? Perhaps due to NaNs or '' or some sort of blank.  If so, the solution should be applied to ch0 also in case it happens in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7b73ac3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [224]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m allwords1 \u001b[38;5;241m=\u001b[39m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m token[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit() ] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m ch1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m allwords1 \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m allwords1 \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "Input \u001b[0;32mIn [224]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m allwords1 \u001b[38;5;241m=\u001b[39m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit() ] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m ch1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m allwords1 \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m allwords1 \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "allwords1 = [[word for word in token[0].split() ] for token in ch1['tokens']]\n",
    "allwords1 = [item for sublist in allwords1 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cd5ca5b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [195]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#count_vec = CountVectorizer(stop_words='english')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#count_occurs = count_vec.fit_transform([doc])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#bow = {}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#    for key in word_index:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#        bow[key] = sequences[0].count(word_index[key])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bow0 \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 7\u001b[0m bow0 \u001b[38;5;241m=\u001b[39m [[sequences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(word_index[key]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokenlist ] \u001b[38;5;28;01mfor\u001b[39;00m tokenlist \u001b[38;5;129;01min\u001b[39;00m ch0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Input \u001b[0;32mIn [195]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#count_vec = CountVectorizer(stop_words='english')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#count_occurs = count_vec.fit_transform([doc])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#bow = {}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#    for key in word_index:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#        bow[key] = sequences[0].count(word_index[key])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bow0 \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 7\u001b[0m bow0 \u001b[38;5;241m=\u001b[39m [[sequences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(word_index[key]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokenlist ] \u001b[38;5;28;01mfor\u001b[39;00m tokenlist \u001b[38;5;129;01min\u001b[39;00m ch0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Input \u001b[0;32mIn [195]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#count_vec = CountVectorizer(stop_words='english')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#count_occurs = count_vec.fit_transform([doc])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#bow = {}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#    for key in word_index:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#        bow[key] = sequences[0].count(word_index[key])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bow0 \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 7\u001b[0m bow0 \u001b[38;5;241m=\u001b[39m [[\u001b[43msequences\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(word_index[key]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokenlist ] \u001b[38;5;28;01mfor\u001b[39;00m tokenlist \u001b[38;5;129;01min\u001b[39;00m ch0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "#count_vec = CountVectorizer(stop_words='english')\n",
    "#count_occurs = count_vec.fit_transform([doc])\n",
    "#bow = {}\n",
    "#    for key in word_index:\n",
    "#        bow[key] = sequences[0].count(word_index[key])\n",
    "bow0 = {}\n",
    "bow0 = [[sequences[0].count(word_index[key]) for word in tokenlist ] for tokenlist in ch0['tokens']]      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba86bc",
   "metadata": {},
   "source": [
    "#### Ok, time to try a spark ML model. I'm working off the example <a href='https://spark.apache.org/docs/latest/ml-pipeline.html#code-examples'>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7710b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move pandas df to spark df and tell it what are the features and what is the label\n",
    "clean_text['target'] = target\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', \"true\")\n",
    "training = spark.createDataFrame(clean_text, [\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de4c81ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features='monkaw cam talk', label=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b05f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d1182ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.StringType$:string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Learn a LogisticRegression model. This uses the parameters stored in lr.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/spark/spark/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/Documents/spark/spark/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/Documents/spark/spark/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/spark/spark/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Documents/spark/spark/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.StringType$:string."
     ]
    }
   ],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
